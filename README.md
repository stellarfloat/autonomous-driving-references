# 2021-1 차량지능기초 과제 1

- `20203155 추헌준`

## 목차

- Datasets
  - Berkeley DeepDrive
  - nuScenes Dataset
  - Waymo Open Dataset
- Open Source Examples
  - 1
  - 2
- Hands-on
  - 1

## Datasets

### [Berkeley DeepDrive](https://bdd-data.berkeley.edu/)

Paper:
[BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning](https://arxiv.org/abs/1805.04687)

- Video Data
  - 도합 1100시간 이상의 고화질 주행 영상
  - 다양한 시간대, 기상 조건, 주행 시나리오들을 포함하는 10만개의 영상
  - GPS, IMU 데이터와 시간 정보 또한 포함

- Road Object Detection
  - bus, traffic light, traffic sign, person, bike, truck, motor, car, train, and rider
  - 10만개의 이미지, 사각형으로 표시됨

<img src="https://user-images.githubusercontent.com/66378218/113841095-56c7f680-97cc-11eb-86dc-40761335e8ad.png" width="400">

- Instance Segmentation
  - 1만개 이상의 각 오브젝트가 표지된 이미지들

<img src="https://user-images.githubusercontent.com/66378218/113841167-69423000-97cc-11eb-892a-99a4bb6e2f1f.png" width="400">

- Driveable Area
  - 이미지에서 주행 가능한 구역을 식별
  - 10만장

<img src="https://user-images.githubusercontent.com/66378218/113841323-8bd44900-97cc-11eb-9025-c4b9d63de89b.png" width="400">

- Lane Markings
  - 차선이 표지된 10만장의 이미지

<img src="https://user-images.githubusercontent.com/66378218/113841344-9262c080-97cc-11eb-9dd9-77ec29ab7ae5.png" width="400">

---

### [nuScenes Dataset](https://www.nuscenes.org/nuscenes)

- 1,000 개의 scenes
- 1.4M 카메라 이미지
- 390k LIDAR sweeps
- 1.4M RADAR sweeps
- 1.4M object bounding boxes in 40k keyframes

<img src="https://user-images.githubusercontent.com/66378218/113851669-b2977d00-97d6-11eb-9ef2-d7f47e269df2.png" width="400">

위와 같은 차량으로 데이터를 수집, 보스턴과 싱가포르에서 15시간이 주행으로 수집하였다고 합니다.

<img src="https://user-images.githubusercontent.com/66378218/113851769-cb079780-97d6-11eb-8ea5-f6408d0ac966.png" width="400">

튜토리얼 노트북 : <a href="https://colab.research.google.com/github/nutonomy/nuscenes-devkit/" target="_blank" rel="noopener noreferrer" style="vertical-align: middle;"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>

---

### [Waymo Open Dataset](https://waymo.com/open/)

Paper:
[Scalability in Perception for Autonomous Driving: Waymo Open Dataset](https://arxiv.org/abs/1912.04838)

Description from : [waymo.com/open/about](https://waymo.com/open/about/)

#### Motion Dataset

- 103,354, 20s 10Hz segments (over 20 million frames), mined for interesting interactions
- 574 hours of data
- Object data
  - 10.8M objects with tracking IDs
  - Labels for 3 object classes - Vehicles, Pedestrians, Cyclists
  - 3D bounding boxes for each object
  - Mined for interesting behaviors and scenarios for behavior prediction research, such as unprotected turns, merges, lane changes, and intersections
  - 3D bounding boxes are generated by a model trained on the Perception Dataset and detailed in our [paper](https://arxiv.org/abs/2103.05073).
- Map data
  - 3D map data for each segment
  - Locations include: San Francisco, Phoenix, Mountain View, Los Angeles, Detroit, and Seattle

#### Perception Dataset

- 1,950 segments of 20s each, collected at 10Hz (390,000 frames) in diverse geographies and conditions
- Sensor data
  - 1 mid-range lidar
  - 4 short-range lidars
  - 5 cameras (front and sides)
  - Synchronized lidar and camera data
  - Lidar to camera projections
  - Sensor calibrations and vehicle poses
- Labeled data
  - Labels for 4 object classes - Vehicles, Pedestrians, Cyclists, Signs
  - High-quality labels for lidar data in 1,200 segments
  - 12.6M 3D bounding box labels with tracking IDs on lidar data
  - High-quality labels for camera data in 1,000 segments
  - 11.8M 2D bounding box labels with tracking IDs on camera data

