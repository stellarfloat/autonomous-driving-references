# 2021-1 차량지능기초 과제 1

`20203155 추헌준`

## 목차

- Datasets
  - Berkeley DeepDrive
  - nuScenes Dataset
  - Waymo Open Dataset
- Open Source Examples
  - Apollo
  - ndrplz/self-driving-car
- Hands-on
  - ndrplz/self-driving-car/lane_finding_basic

## Datasets

### [Berkeley DeepDrive](https://bdd-data.berkeley.edu/)

Paper:
[BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning](https://arxiv.org/abs/1805.04687)

- Video Data
  - 도합 1100시간 이상의 고화질 주행 영상
  - 다양한 시간대, 기상 조건, 주행 시나리오들을 포함하는 10만개의 영상
  - GPS, IMU 데이터와 시간 정보 또한 포함

- Road Object Detection
  - bus, traffic light, traffic sign, person, bike, truck, motor, car, train, and rider
  - 10만개의 이미지, 사각형으로 표시됨

<img src="https://user-images.githubusercontent.com/66378218/113841095-56c7f680-97cc-11eb-86dc-40761335e8ad.png" width="400">

- Instance Segmentation
  - 1만개 이상의 각 오브젝트가 표지된 이미지들

<img src="https://user-images.githubusercontent.com/66378218/113841167-69423000-97cc-11eb-892a-99a4bb6e2f1f.png" width="400">

- Driveable Area
  - 이미지에서 주행 가능한 구역을 식별
  - 10만장

<img src="https://user-images.githubusercontent.com/66378218/113841323-8bd44900-97cc-11eb-9025-c4b9d63de89b.png" width="400">

- Lane Markings
  - 차선이 표지된 10만장의 이미지

<img src="https://user-images.githubusercontent.com/66378218/113841344-9262c080-97cc-11eb-9dd9-77ec29ab7ae5.png" width="400">

---

### [nuScenes Dataset](https://www.nuscenes.org/nuscenes)

- 1,000 개의 scenes
- 1.4M 카메라 이미지
- 390k LIDAR sweeps
- 1.4M RADAR sweeps
- 1.4M object bounding boxes in 40k keyframes

<img src="https://user-images.githubusercontent.com/66378218/113851669-b2977d00-97d6-11eb-9ef2-d7f47e269df2.png" width="400">

위와 같은 차량으로 데이터를 수집, 보스턴과 싱가포르에서 15시간이 주행으로 수집하였다고 합니다.

<img src="https://user-images.githubusercontent.com/66378218/113851769-cb079780-97d6-11eb-8ea5-f6408d0ac966.png" width="400">

튜토리얼 노트북 : <a href="https://colab.research.google.com/github/nutonomy/nuscenes-devkit/" target="_blank" rel="noopener noreferrer" style="vertical-align: middle;"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>

---

### [Waymo Open Dataset](https://waymo.com/open/)

Paper:
[Scalability in Perception for Autonomous Driving: Waymo Open Dataset](https://arxiv.org/abs/1912.04838)

Description from : [waymo.com/open/about](https://waymo.com/open/about/)

#### Motion Dataset

- 103,354, 20s 10Hz segments (over 20 million frames), mined for interesting interactions
- 574 hours of data
- Object data
  - 10.8M objects with tracking IDs
  - Labels for 3 object classes - Vehicles, Pedestrians, Cyclists
  - 3D bounding boxes for each object
  - Mined for interesting behaviors and scenarios for behavior prediction research, such as unprotected turns, merges, lane changes, and intersections
  - 3D bounding boxes are generated by a model trained on the Perception Dataset and detailed in our [paper](https://arxiv.org/abs/2103.05073).
- Map data
  - 3D map data for each segment
  - Locations include: San Francisco, Phoenix, Mountain View, Los Angeles, Detroit, and Seattle

#### Perception Dataset

- 1,950 segments of 20s each, collected at 10Hz (390,000 frames) in diverse geographies and conditions
- Sensor data
  - 1 mid-range lidar
  - 4 short-range lidars
  - 5 cameras (front and sides)
  - Synchronized lidar and camera data
  - Lidar to camera projections
  - Sensor calibrations and vehicle poses
- Labeled data
  - Labels for 4 object classes - Vehicles, Pedestrians, Cyclists, Signs
  - High-quality labels for lidar data in 1,200 segments
  - 12.6M 3D bounding box labels with tracking IDs on lidar data
  - High-quality labels for camera data in 1,000 segments
  - 11.8M 2D bounding box labels with tracking IDs on camera data

## Open Source Examples

### [Apollo](https://github.com/ApolloAuto/apollo)

Apollo는 자율 주행 자동차들의 개발, 테스트, 배포를 도와주는 고성능이면서 유연한 아키텍쳐라고 합니다.

#### Prerequisites

- The vehicle equipped with the by-wire system, including but not limited to brake-by-wire, steering-by-wire, throttle-by-wire and shift-by-wire (Apollo is currently tested on Lincoln MKZ)
- A machine with a 8-core processor and 16GB memory minimum
- NVIDIA Turing GPU is strongly recommended
- Ubuntu 18.04
- NVIDIA driver version 455.32.00 and above (Web link)
- Docker-CE version 19.03 and above
- NVIDIA Container Toolkit

Apollo가 아무래도 실제 자율주행 차량에 적용하도록 만들어진 플랫폼이다 보니, 실제 by-wire system이 탑재된 자동차가 필요하다는 것이 인상 깊었습니다.

#### Architecture

- Apollo 6.0

<img src="https://github.com/ApolloAuto/apollo/blob/master/docs/demo_guide/images/Apollo_6_0.png" width="400">

- Hardware Connection Overview

<img src="https://github.com/ApolloAuto/apollo/raw/master/docs/demo_guide/images/Hardware_connection_3_5_1.png
" width="400">

- Software Overview

<img src="https://github.com/ApolloAuto/apollo/raw/master/docs/demo_guide/images/Apollo_3_5_software_architecture.png
" width="400">

---

### [ndrplz/self-driving-car](https://github.com/ndrplz/self-driving-car)

Udacity Self-Driving Car Engineer Nanodegree 에 등장하는 코드들을 구현해 놓은 레포입니다. 

#### P1: Basic Lane Finding

이 코드의 목적은 기본적으로 차 정면에서 찍힌 이미지에서 차선을 식별하는 일련의 과정을 파이프라인화 하는 것입니다. 인식 과정에서 Neural Network는 사용되지 않고, 전통적인 컴퓨터 비전 기술들을 사용하여 간단한 인식을 수행하게 됩니다.

먼저, 대략적인 실행의 흐름은 다음과 같습니다.

> main.py
```python
in_image = cv2.cvtColor(cv2.imread(test_img, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)
out_image = color_frame_pipeline([in_image], solid_lines=True)
cv2.imwrite(out_path, cv2.cvtColor(out_image, cv2.COLOR_RGB2BGR))
```

`main.py`는 대상 이미지를 읽어서 `color_frame_pipeline()`을 호출하고, 리턴값을 받아 저장하는 간단한 구조로 이루어져 있습니다.

`color_frame_pipeline()`은 `lane_detection.py`에 선언되어 있는데, 이 함수에서 `get_lane_lines()`을 호출하여 차선을 감지하고 이 데이터를 받아 받은 이미지에 합성하여 리턴합니다. 

`get_lane_lines()`에서는 기계가 알아서 추출하는 것이 아닌 사람이 직접 설정한 기준을 활용해 차선을 추출합니다. 그 과정은 대략 다음과 같습니다.
```
흑백 변환 -> 이미지 블러 처리 -> edge detection -> hough transform -> 30도에서 60도 사이에 있는 직선 추출
```


이러한 과정을 거쳐 다음과 같은 결과가 도출됩니다.

- 입력 이미지

<img src="https://user-images.githubusercontent.com/66378218/113875631-efbd3880-97f1-11eb-83d5-1614ca2b5483.jpg
" width="400">

- 출력 이미지

<img src="https://user-images.githubusercontent.com/66378218/113875779-11b6bb00-97f2-11eb-9c29-ce320d25cdad.jpg
" width="400">

## Hands-on

### ndrplz/self-driving-car/lane_finding_basic

> 1번 예제를 선택한 이유

레포에 있는 일부 다른 예제들에는 tensorflow를 활용하여 좀 더 심화된 학습을 수행하기도 하지만, tensorflow 2 출시에 따라 breaking change가 다수 존재하여 구 버전을 기준으로 작성된 이 코드들과는 호환성 문제가 발생하였습니다. 따라서 지금까지도 호환성이 유지되는 라이브러리(opencv)를 사용한 이 예제를 선택하였습니다.

#### 구현 환경

- Prerequisites
  - opencv-python
  - matplotlib

#### 발생한 이슈

`cv2.cvtColor()`에서 파일 임포트 관련 문제가 발생하였습니다.

```text
cv2.error: OpenCV(4.5.1) color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'
```

파일 경로에 한글이 들어가서 그런 것으로 추정되어서, 경로에 ASCII 문자만 존재하도록 프로젝트 폴더를 이동하였습니다. 그 결과, 예상대로 문제가 해결되었습니다.

#### 개선점

스크립트가 실행될 때, 기본적으로 경로 기준이 현재 작업 디렉토리로 잡히고 있었습니다. 이는 이미지 데이터들을 읽고 쓸 때 오류가 발생할 수도 있으므로 파일 I/O를 현재 작업 디렉토리와 독립적으로 작동하도록 수정하였습니다.
파이썬 파일을 기준으로 경로 상수를 미리 저장하고, `os.path.join()`으로 상대적인 경로를 지정할 수 있었습니다.

```python
MAIN_DIR = os.path.split(os.path.abspath(__file__))[0]
```

#### 디렉토리 구조

```text
.
├── Line.py
├── data
│   ├── test_images
│   └── test_videos
├── lane_detection.py
├── main.py
└── out
    ├── images
    └── videos
```

#### 실행

```text
python main.py
```
